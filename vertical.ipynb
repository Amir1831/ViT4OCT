{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64abed7-cdf0-485d-a067-bbf94b7ada48",
   "metadata": {
    "id": "f64abed7-cdf0-485d-a067-bbf94b7ada48"
   },
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "BATCH_SIZE = 64                     # Number of images processed in one training batch\n",
    "IMG_SIZE = (128, 128)               # Size of input images (height, width)\n",
    "PATCH_SIZE = (8, 8)                 # Size of each patch extracted from images\n",
    "\n",
    "# Calculate the number of patches extracted from the entire image\n",
    "NUM_PATCH = (IMG_SIZE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# Downsampling factor (e.g., after feature extraction or pooling)\n",
    "T = NUM_PATCH // 4\n",
    "\n",
    "# Transformer Configuration\n",
    "NUM_H = 4                           # Number of attention heads in the Transformer\n",
    "NUM_TR = 11                         # Number of Transformer blocks/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe46d77-6c4d-4102-9994-2cb89f633f18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebe46d77-6c4d-4102-9994-2cb89f633f18",
    "outputId": "94ed8309-10d5-4a18-8204-d19f038e8f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.1+cu121\n",
      "torchvision version: 0.16.1+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960eb156-c1b1-4e76-a812-01bf045835bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "960eb156-c1b1-4e76-a812-01bf045835bd",
    "outputId": "32499e4f-2442-4247-97c3-c15ebe830f48"
   },
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "import data_setup, engine\n",
    "from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e246f92-e509-474e-b6c7-c82cf11cb8ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5e246f92-e509-474e-b6c7-c82cf11cb8ca",
    "outputId": "fdeb0c6d-744b-4d60-c90b-116de314d5db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "t7uI7hBS_uic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7uI7hBS_uic",
    "outputId": "c8bb9253-4c7d-4c78-9f65-71556a05e655"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to a data folder\n",
    "data_path = Path(\"../\")\n",
    "image_path = data_path / \"OCT2017\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a426b6-df22-4a58-9d5e-b382c73c6048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92a426b6-df22-4a58-9d5e-b382c73c6048",
    "outputId": "77ca65ed-b28f-4fe7-c62e-8eb172c34381"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('../OCT2017/train'), PosixPath('../OCT2017/test'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sA7cvO3T57vA",
   "metadata": {
    "id": "sA7cvO3T57vA"
   },
   "source": [
    "**************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "-e6jIWBp68Od",
   "metadata": {
    "id": "-e6jIWBp68Od"
   },
   "outputs": [],
   "source": [
    "my_transform = transforms.Compose([\n",
    "                                    transforms.Resize((IMG_SIZE)),\n",
    "                                    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6OTCCGLs9flA",
   "metadata": {
    "id": "6OTCCGLs9flA"
   },
   "outputs": [],
   "source": [
    "# Original img_to_patch function\n",
    "def generate_vertical_permutation(H_p, W_p):\n",
    "    \"\"\"\n",
    "    Generates a permutation list that rearranges patches in vertical order.\n",
    "    Inputs:\n",
    "        H_p - Number of patches along the height (rows)\n",
    "        W_p - Number of patches along the width (columns)\n",
    "    Returns:\n",
    "        perm - List of indices representing the vertical order\n",
    "    \"\"\"\n",
    "    perm = []\n",
    "    for j in range(W_p):  # For each column\n",
    "        for i in range(H_p):  # For each row\n",
    "            idx = i * W_p + j  # Calculate index in the flattened array\n",
    "            perm.append(idx)\n",
    "    return perm\n",
    "\n",
    "\n",
    "def img_to_patch_vertical(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Extracts patches from images in vertical order, column by column.\n",
    "\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of an image grid.\n",
    "\n",
    "    Returns:\n",
    "        x - Tensor of extracted patches in the desired vertical order.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    H_p = H // patch_size\n",
    "    W_p = W // patch_size\n",
    "\n",
    "    # Ensure H and W are divisible by patch_size\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"H and W must be divisible by patch_size\"\n",
    "\n",
    "    # Reshape and permute to extract patches\n",
    "    x = x.reshape(B, C, H_p, patch_size, W_p, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)  # [B, H_p, W_p, C, p_H, p_W]\n",
    "    x = x.reshape(B, H_p * W_p, C, patch_size, patch_size)  # [B, H_p * W_p, C, p_H, p_W]\n",
    "\n",
    "    # Generate permutation for vertical ordering\n",
    "    perm = generate_vertical_permutation(H_p, W_p)\n",
    "    perm_tensor = torch.tensor(perm, device=x.device)\n",
    "\n",
    "    # Rearrange patches according to the vertical order\n",
    "    x = x[:, perm_tensor, ...]  # Reorder patches along dimension 1\n",
    "\n",
    "    if flatten_channels:\n",
    "        # Flatten the C, p_H, p_W dimensions into a single feature vector per patch\n",
    "        x = x.flatten(2, 4)  # [B, Num_Patches, C * p_H * p_W]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e2dda6-8af0-4255-815f-4d885fa4b477",
   "metadata": {
    "id": "b8e2dda6-8af0-4255-815f-4d885fa4b477"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "JIQM39VN6HnC",
   "metadata": {
    "id": "JIQM39VN6HnC"
   },
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super().__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim1, self.dim2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "DM62A0Aw6IQV",
   "metadata": {
    "id": "DM62A0Aw6IQV"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim,n_feature, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv3d(3 , n_feature*3 , kernel_size = 3 , padding=1 , stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2,1,1) , stride=(2,1,1) , padding=(0,0,0)),\n",
    "\n",
    "            nn.Conv3d(n_feature*3 , n_feature*6 , 3 , padding=1 , stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2,1,1) , stride=(2,1,1) , padding=(0,0,0)),\n",
    "            nn.Conv3d(n_feature*6 , n_feature*8 , 3 , padding=1 , stride=1),\n",
    "\n",
    "            Transpose(2,1),\n",
    "            nn.Flatten(2,-1),\n",
    "            nn.Linear(n_feature*8*PATCH_SIZE[0]*PATCH_SIZE[0] ,embed_dim )\n",
    "\n",
    "\n",
    "        )\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+int(NUM_PATCH / 4),embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch_vertical(x, self.patch_size , flatten_channels=False).transpose(1,2)\n",
    "        B, _, _ , _ , _= x.shape\n",
    "        T = int(NUM_PATCH / 4)\n",
    "        x = self.input_layer(x)\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8iDaZ7uu6IWQ",
   "metadata": {
    "id": "8iDaZ7uu6IWQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AmirModel = VisionTransformer(embed_dim = 256 ,\n",
    "                            hidden_dim = 512,\n",
    "                            n_feature=8 ,\n",
    "                            num_channels = 3,\n",
    "                            num_heads = NUM_H ,\n",
    "                            num_layers = NUM_TR ,\n",
    "                            num_classes=4,\n",
    "                            patch_size=PATCH_SIZE[0] ,\n",
    "                            num_patches=NUM_PATCH ,\n",
    "                            dropout=0.1 ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p754GQyt5_s4",
   "metadata": {
    "id": "p754GQyt5_s4"
   },
   "source": [
    "**************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fbd83a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "8fbd83a1",
    "outputId": "b441936c-44a0-4d42-9ab3-d8930d92c475",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "VisionTransformer (VisionTransformer)         [32, 3, 128, 128]    [32, 4]              16,896               True\n",
       "├─Sequential (input_layer)                    [32, 3, 256, 8, 8]   [32, 64, 256]        --                   True\n",
       "│    └─Conv3d (0)                             [32, 3, 256, 8, 8]   [32, 24, 256, 8, 8]  1,968                True\n",
       "│    └─ReLU (1)                               [32, 24, 256, 8, 8]  [32, 24, 256, 8, 8]  --                   --\n",
       "│    └─MaxPool3d (2)                          [32, 24, 256, 8, 8]  [32, 24, 128, 8, 8]  --                   --\n",
       "│    └─Conv3d (3)                             [32, 24, 128, 8, 8]  [32, 48, 128, 8, 8]  31,152               True\n",
       "│    └─ReLU (4)                               [32, 48, 128, 8, 8]  [32, 48, 128, 8, 8]  --                   --\n",
       "│    └─MaxPool3d (5)                          [32, 48, 128, 8, 8]  [32, 48, 64, 8, 8]   --                   --\n",
       "│    └─Conv3d (6)                             [32, 48, 64, 8, 8]   [32, 64, 64, 8, 8]   83,008               True\n",
       "│    └─Transpose (7)                          [32, 64, 64, 8, 8]   [32, 64, 64, 8, 8]   --                   --\n",
       "│    └─Flatten (8)                            [32, 64, 64, 8, 8]   [32, 64, 4096]       --                   --\n",
       "│    └─Linear (9)                             [32, 64, 4096]       [32, 64, 256]        1,048,832            True\n",
       "├─Dropout (dropout)                           [32, 65, 256]        [32, 65, 256]        --                   --\n",
       "├─Sequential (transformer)                    [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    └─AttentionBlock (0)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (1)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (2)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (3)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (4)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (5)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (6)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (7)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (8)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (9)                     [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "│    └─AttentionBlock (10)                    [65, 32, 256]        [65, 32, 256]        --                   True\n",
       "│    │    └─LayerNorm (layer_norm_1)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─MultiheadAttention (attn)         [65, 32, 256]        [65, 32, 256]        263,168              True\n",
       "│    │    └─LayerNorm (layer_norm_2)          [65, 32, 256]        [65, 32, 256]        512                  True\n",
       "│    │    └─Sequential (linear)               [65, 32, 256]        [65, 32, 256]        262,912              True\n",
       "├─Sequential (mlp_head)                       [32, 256]            [32, 4]              --                   True\n",
       "│    └─LayerNorm (0)                          [32, 256]            [32, 256]            512                  True\n",
       "│    └─Linear (1)                             [32, 256]            [32, 4]              1,028                True\n",
       "=============================================================================================================================\n",
       "Total params: 6,981,540\n",
       "Trainable params: 6,981,540\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.30\n",
       "=============================================================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 506.99\n",
       "Params size (MB): 16.28\n",
       "Estimated Total Size (MB): 529.56\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=AmirModel,\n",
    "        input_size=(32, 3, 128, 128), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2f58ff-6182-453a-a802-70ff98c09557",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd2f58ff-6182-453a-a802-70ff98c09557",
    "outputId": "667cdfc2-893f-486a-d6da-2f0870facd41"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                test_dir=test_dir,\n",
    "                                                                                transform=my_transform,\n",
    "                                                                                batch_size=BATCH_SIZE,\n",
    "                                                                                num_workers = 8) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbab23e-0ffd-46ab-99e5-6cb39d2196ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adbab23e-0ffd-46ab-99e5-6cb39d2196ad",
    "outputId": "1bbb3322-b91c-46d7-935f-b23714784db4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNV', 'DME', 'DRUSEN', 'NORMAL']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6kiQYAXZDkNa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6kiQYAXZDkNa",
    "outputId": "88c6f080-663f-465e-daf3-6f444df100fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dir: ../OCT2017/train\n",
      "train_dir: ../OCT2017/test\n",
      "len train_dataloader: 1305\n",
      "len test_dataloader: 16\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dir: {train_dir}\")\n",
    "print(f\"train_dir: {test_dir}\")\n",
    "print(f\"len train_dataloader: {len(train_dataloader)}\")\n",
    "print(f\"len test_dataloader: {len(test_dataloader)}\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57bb3c01-547f-480e-b985-b915c0634c93",
   "metadata": {
    "id": "57bb3c01-547f-480e-b985-b915c0634c93"
   },
   "outputs": [],
   "source": [
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.SGD(AmirModel.parameters() , 0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee6c9e94-f0bc-4ae3-a4bb-a5121150331d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee6c9e94-f0bc-4ae3-a4bb-a5121150331d",
    "outputId": "7190bbd9-d964-4a43-bd95-3313e1331956",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "torch.set_float32_matmul_precision('high')\n",
    "AmirModel = torch.compile(AmirModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a49408b4-24d9-4bb1-90a2-dd61c08f78a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "referenced_widgets": [
      "f37b4b6a96124fcba15fb5567800d404",
      "6e2ab6f2c1284dcc82493761ee6e275d",
      "a145de692b604af49dfb2acf21a7ee0f",
      "a07affbbcd73486f802e6f971f5c97e2",
      "e2f5dc6e434944b888efefe60f859cd9",
      "ba3ef09352de47959d0fd7f3aa6fd0db",
      "58af7bf0e0084c4694f5d9b0eacc4e90",
      "d092ebd4a850416f9073ae4edd00d872",
      "bfaa83d982a440478bc69388c9044f0c",
      "84147fc5fd244930af67b80477d67cbd",
      "76e232abd44e495b90f29030c8a03060"
     ]
    },
    "id": "a49408b4-24d9-4bb1-90a2-dd61c08f78a4",
    "outputId": "63266da6-8806-4e2c-fa81-5c7066d1e924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                | 0/20 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Train the classifier head of the pretrained ViT feature extractor model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m set_seeds()\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAmirModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mPivot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mPart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43msave_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/Vertical/engine.py:175\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, Pivot, Part, save_weights, epochs, device)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 175\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    181\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    182\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    183\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[0;32m/app/Vertical/engine.py:64\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Calculate and accumulate accuracy metric across all batches\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     y_pred_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43my_pred_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_pred)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Adjust metrics to get average loss and accuracy per batch \u001b[39;00m\n\u001b[1;32m     67\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "part = \"2\"\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "set_seeds()\n",
    "x = engine.train(model=AmirModel,\n",
    "                                      train_dataloader=train_dataloader,\n",
    "                                      test_dataloader=test_dataloader,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=20,\n",
    "                                      Pivot=90,\n",
    "                                      Part=part,\n",
    "                                      save_weights=False,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df7bda-abee-4b84-a321-4569d52f6c51",
   "metadata": {
    "id": "18df7bda-abee-4b84-a321-4569d52f6c51"
   },
   "outputs": [],
   "source": [
    "+name = f\"OPT = AdamW ,Compile = True , Epo = 10 ,lr = 0.001 ,least_time =253 ,IMG = 128 ,patch = 8 ,BATCH = 64 ,using_pretrain = True ,Part = 8 , tr_acc = 99.45 , te_acc = 98.93\"\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33f2d5-265e-4891-89fe-102e78151db1",
   "metadata": {
    "id": "ce33f2d5-265e-4891-89fe-102e78151db1"
   },
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad35f6-7e99-4f90-b3da-ee5a18031e1a",
   "metadata": {
    "id": "1fad35f6-7e99-4f90-b3da-ee5a18031e1a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"./drive/MyDrive/TrackExp/\" + name , x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd65d61-3a2b-4e2a-abf5-965e2ec52dfd",
   "metadata": {
    "id": "acd65d61-3a2b-4e2a-abf5-965e2ec52dfd"
   },
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5862dc-1962-4a22-a863-7f00a1828526",
   "metadata": {
    "id": "ba5862dc-1962-4a22-a863-7f00a1828526"
   },
   "outputs": [],
   "source": [
    "load_name = f\"OPT = AdamW ,Compile = True , Epo = 20 ,lr = 0.001 ,least_time = 261 ,IMG = 128 ,patch = 8 ,BATCH = 64 ,using_pretrain = False ,Part = 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619e6fe-a308-4f23-8971-7a1e1bb5b071",
   "metadata": {
    "id": "6619e6fe-a308-4f23-8971-7a1e1bb5b071"
   },
   "outputs": [],
   "source": [
    "loaded_results = np.load(\"./TrackExp/\" + name + \".npy\" , allow_pickle='TRUE')\n",
    "loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G_sIF0FCBbTz",
   "metadata": {
    "id": "G_sIF0FCBbTz"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "model_copy = copy.deepcopy(AmirModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706893e-8fa9-4ba5-b0e6-962123259b82",
   "metadata": {
    "id": "8706893e-8fa9-4ba5-b0e6-962123259b82"
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd00943-01aa-4ef4-b366-3cb859a25b6f",
   "metadata": {
    "id": "0fd00943-01aa-4ef4-b366-3cb859a25b6f"
   },
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "from going_modular.going_modular import utils\n",
    "\n",
    "utils.save_model(model=AmirModel,\n",
    "                 target_dir=\"./TrackExp\",\n",
    "                 model_name=name2 + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a6fd9-1340-445f-afc4-70baa2311bb7",
   "metadata": {
    "id": "5c0a6fd9-1340-445f-afc4-70baa2311bb7"
   },
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e22a3c21-f317-4bba-9ea2-5d697e20604c",
   "metadata": {
    "id": "e22a3c21-f317-4bba-9ea2-5d697e20604c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"TrackExp/BestWeight_Part_19_Epoch_2_TrainLoss_0.01639417985006382_TestLoss_0.027976555515579093TrainAcc_0.9941451149425288_Testacc_0.9951171875.pth\"\n",
    "AmirModel.load_state_dict(torch.load(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64f145-ee1e-4d53-80ef-f80a3f892acd",
   "metadata": {},
   "source": [
    "# Save Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f390baac-429f-4627-bbc9-289ec0d8f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(optimizer.state_dict(),\"./TrackOptim/SGDOptimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06438bc-0366-437e-b9f3-0004f173b180",
   "metadata": {},
   "source": [
    "# load Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d44a2616-ed48-44f7-a806-a2526782634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.load_state_dict(torch.load(\"./TrackOptim/SGDOptimizer\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "110a4ad9b3b23ac6a757cfb6c77f1e39e8d0496598f07ec14a944919c025e818"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "58af7bf0e0084c4694f5d9b0eacc4e90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e2ab6f2c1284dcc82493761ee6e275d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3ef09352de47959d0fd7f3aa6fd0db",
      "placeholder": "​",
      "style": "IPY_MODEL_58af7bf0e0084c4694f5d9b0eacc4e90",
      "value": "  0%"
     }
    },
    "76e232abd44e495b90f29030c8a03060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84147fc5fd244930af67b80477d67cbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07affbbcd73486f802e6f971f5c97e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84147fc5fd244930af67b80477d67cbd",
      "placeholder": "​",
      "style": "IPY_MODEL_76e232abd44e495b90f29030c8a03060",
      "value": " 0/10 [00:00&lt;?, ?it/s]"
     }
    },
    "a145de692b604af49dfb2acf21a7ee0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d092ebd4a850416f9073ae4edd00d872",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bfaa83d982a440478bc69388c9044f0c",
      "value": 0
     }
    },
    "ba3ef09352de47959d0fd7f3aa6fd0db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfaa83d982a440478bc69388c9044f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d092ebd4a850416f9073ae4edd00d872": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2f5dc6e434944b888efefe60f859cd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f37b4b6a96124fcba15fb5567800d404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e2ab6f2c1284dcc82493761ee6e275d",
       "IPY_MODEL_a145de692b604af49dfb2acf21a7ee0f",
       "IPY_MODEL_a07affbbcd73486f802e6f971f5c97e2"
      ],
      "layout": "IPY_MODEL_e2f5dc6e434944b888efefe60f859cd9"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
